{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8373dd31-dc17-4d80-abb9-57a917c2fd72",
   "metadata": {},
   "source": [
    "## 기본적인 prompt 구조 이해\n",
    "prompot에는 3가지 종류의 역할이 존재\n",
    "1. **System prompt**  : 사용자 prompt를 입력받기 이전에 정의되는 전제 및 규칙 prompt\n",
    "2. **User prompt** : 사용자가 GPT모델에게 실제로 전달하는 질의 prompt\n",
    "3. **Assistant prompot** : GPT모델이 응답하는 prompt\n",
    "\n",
    "**System prompt란?**\n",
    "- user prompt를 GPT모델에게 전달하기 전 관련된 맥락이나 응답 지침 등을 설정하기 위해 사용\n",
    "- System prompt 옛;\n",
    "  - 출력 형태 저장(ex. JSON, 일발 자연어, 파일 등)\n",
    "  - 페르소나(투자 전문가, 예술가 등) 및 어조 설정(공손한, 전문적인 등)\n",
    "  - 모델이 지켜야 할 규칙들 설정\n",
    "  - 기타 base가 되는 외부 정보 및 지식 주입\n",
    "\n",
    "개발자들이 언어모델의 성능을 최대로 끌어내기 위해 사용자의 질문을 모델이 이해하기 쉽게 변환시켜주는 용도로도 사용하며 ChatGPT포함 웬만한 LLM모델 서비스들은 기본적으로 system prompt가 붙어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5e2db9-81c0-4cd3-a075-4c5d468f5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f965ffcc-db41-4c69-b273-621fa3176c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key : ········\n"
     ]
    }
   ],
   "source": [
    "MY_API_KEY = getpass.getpass(\"OpenAI API Key :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6e8acd-a20c-4f63-8797-ae8b820e7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a2b54e-69b5-43bb-b609-71d390437c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 왜 하늘색일까요? 그 이유는 바로 태양 빛이 하늘을 통과할 때 일어나는 일 때문이에요. 태양은 모든 색깔의 빛을 내뿜는데, 그 중에서도 파란색 빛이 하늘에 가장 많이 퍼지게 되어요. 그래서 우리 눈에 보이는 하늘은 파란색으로 보이게 되는 거죠! 이렇게 하늘은 파란색으로 보이게 되는 이유 때문에 우리는 하늘을 '하늘색'이라고 부르게 되었어요.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role':'system', 'content':'당신은 물리학 선생님 입니다. 초등학생에게 설명하듯 아주 쉽고 친근하게 설명해야 합니다.'},\n",
    "               {'role':'user', 'content':'왜 하늘은 하늘색 인가요?'}\n",
    "               ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ef1b5a-cd52-4d70-bf4c-bdedb7daa1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 하늘색인 이유는 바로 '레이리'라는 현상 때문이에요! 레이리는 태양빛이 하늘을 통과할 때 빛의 파장이 짧은 파란색 빛이 더 많이 흩어지기 때문에 하늘이 파란색으로 보이는 거예요. 파란색 빛이 더 많이 흩어지기 때문에 하늘은 파란색으로 보이게 되는 거죠!\n"
     ]
    }
   ],
   "source": [
    "# System prompt를 지정해줄 수 있지만 user prompt에 내용을 같이 작성해도 무방항\n",
    "question = '''당신은 물리학 선생님 입니다. 초등학생에게 설명하듯 아주 쉽고 친근하게 설명해야 합니다. \n",
    "왜 하늘은 하늘색 인가요?'''\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role':'user', 'content':question}\n",
    "               ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23aafeb-2fc7-469f-b66c-ddaf85a6cc92",
   "metadata": {},
   "source": [
    "### Stream 객체\n",
    "- stream=True로 지정시 GPT가 문장을 모두 완성하여 출력하기 전에 각 토큰별로 완성되는데로 바로바로 보게 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a69d813-0f27-4031-b35a-463bf179ba4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 하늘색인 이유는 대기 중의 분자들이 햇빛을 흡수하고 산란시키기 때문입니다. 태양으로부터 오는 빛은 다양한 파장을 가지고 있는데, 대기 중의 분자들은 이 빛을 흡수하고 다시 방출하면서 파장이 긴 파장인 빨간색과 주황색은 흡수되고 파장이 짧은 파란색과 보라색은 산란되어 눈에 보이게 됩니다. 이러한 현상으로 인해 하늘은 파란색으로 보이게 되는 것입니다."
     ]
    }
   ],
   "source": [
    "completion_stream = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role':'user', 'content':'왜 하늘은 하늘색인가요?'}\n",
    "               ],\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# stream을 설정하게 되면 출력 형태가 ChatCompletionChunk객체로 변환되고 각 토큰들이 하나씩 생성되어 있는 것을 볼 수 있음\n",
    "for i in completion_stream :\n",
    "    #print(i)\n",
    "    content = i.choices[0].delta.content    # 실제 응답 토큰 추출\n",
    "    if content is not None :   # content에 내용이 들어있다면\n",
    "        print(content, end='') # 이어붙여서 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943624d-24ea-405e-9649-ce5fababd66b",
   "metadata": {},
   "source": [
    "### 프롬프트 엔지니어링에서 기본적으로 생각해야할 점\n",
    "1. GPT모델의 출력값은 입력값에 대한 의존도가 매우 높음\n",
    "   - 잘 입력할 것 같은데 원하는 결과가 나오지 않는다면 입력이 모호하거나 응답에 필요한 내용이 빠졌을 수 있음\n",
    "   - 그게 아닌 경우라면 해당 모델의 성능으로 해결하기 힘든 요청일 수도 있음\n",
    "2. 자연어 질의를 기반으로 하기 때문에 절대적으로 성능 좋은 prompt를 단정지을 수 없음\n",
    "   - 입력 문장에 대한 문맥을 파악하고 새로운 문장을 생성할 때 그때그때 내부 연산이 달라질 수 있음\n",
    "   - 대화에서 성능이 좋다는 것은 사용자가 만족해야한다는 것인데 이는 매우 주관적이고 판단하기 힘듦\n",
    "   - 즉, 프롬프트 엔지니어링은 task에 맞는 여러번의 테스트가 필수적이고 이를 통한 반복적인 개선이 수반되어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02942f03-0c17-4c00-b442-fe92885df9ef",
   "metadata": {},
   "source": [
    "## 그러면 LLM 평가는 어떻게 진행해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70aff86-a9ee-4f49-a670-e60f7550cf32",
   "metadata": {},
   "source": [
    "### 전통적인 Language Model의 평가 지표\n",
    "1. MMLU(Massive Multitask Language Unederstading)\n",
    "   - 다양한 분양에 대해 질문 후 정답을 찾아내는 객관식 시험으로 평가\n",
    "2. HellaSwag\n",
    "   - 문장들을 주고 이어지는 마지막 문장으로 가장 적합한 문장들 4개 중 하나를 고르는 문제로 평가\n",
    "3. TruthfulQA\n",
    "   - 할루시네이션 측정용 데이터셋을 활용하며 주어진 문제에 대한 답이 맞는지 정확도를 측정하여 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df01ee6-f9b6-4de2-b7cf-096957e614f3",
   "metadata": {},
   "source": [
    "#### 위 전통적인 평가지표들을 사용하여 평가할 수도 있지만 현재 우리가 진행하려는 Q&A task에 적합하다고 보기에는 애매하여 다른 평가 기준 설정이 필요함\n",
    "- 일반적으로 실제 자연어 서비스에서 해당 지표들만으로 점수가 높다고 바로 사용하지는 않고 실제 사용자 평가를 추가적으로 진행 후 적용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb70f19-397a-4d96-8a42-b3f4a3928f1e",
   "metadata": {},
   "source": [
    "### Q&A task에 적합한 평가 방식\n",
    "1. **Human Based Evaluation** - 사람이 직접 평가하는 방식\n",
    "2. **Model Based Evaluation** - LLM이 평가하는 방식\n",
    "3. **Code Based Evaluation** - 코드와 지표로 평가하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f5255-1443-4c5d-9989-52d3c5c6ba20",
   "metadata": {},
   "source": [
    "### 1) Human Based Evaluation\n",
    "- 전문가 블라인드 테스트(각기 다른 LLM의 여러 답변 중에서 더 좋은 답변을 사람이 선택)\n",
    "- 명확한 결과로 성능을 판단하기 쉬움\n",
    "- 많은 인력에 따른 비용과 시간이 필요함\n",
    "#### LMSys사의 Chatbot Arena 평가\n",
    "- 대표적인 Human Based 평가 방법 중 하나로 동일한 질문에 대해 2개의 모델의 답변을 보고 승/패/무 투표 이후 모델명을 공개하는 방식\n",
    "- 사이트 : https://chat.lmsys.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16373517-c805-459f-b73c-1d777d5b054e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2) Model Based Evaluation\n",
    "고성능 LLM을 통해 평가하는 방법(일반적으로 GPT_4o 이상급)\n",
    "- 실제로 사람이 평가하는 것과 굉장히 유사한 성능이라는 논문 결과들이 나오고 있음\n",
    "평가 방식에는 3가지가 존재\n",
    "1. **Pairwise Comparison**\n",
    "   - 2개의 평가받은 모델에 같은 질문을 하고, 고성능 모델이 2개의 답변을 받아 둘 중 어떤 답변이 더 좋은지 또는 무승부인지 출력\n",
    "2. **Single Answer Grading**\n",
    "   - 질문과 답변이 있을 때 답변에 점수를 매기는 것\n",
    "3. **Refernece-Guided Grading**\n",
    "   - 예시 답변을 주고 이와 비교하여 +,-로 상대적인 점수를 매기는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930f7e3-4792-4505-81d2-0ef227059c3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3) Code Based Evaluation\n",
    "- 우리에게 익숙한 코드/로직을 통한 평가법\n",
    "  - Accuracy, Precision, Recall, F1Score\n",
    "  - ROUGE(Recall-Oriented Understudy for Gisting Evaluation)  : 요약 및 자연어 생성을 평가하는 지표\n",
    "  - BLUE(Bilingual Evaluation Underatudy) : 번역, 자연어 생성 등을 평가\n",
    "  - 단 Huamn Based나 Model Based에 비해 실제 사용자의 만족과는 다소 거리가 있을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a108d26-902c-4d48-969e-15a0e8234886",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM benchmark 플랫폼 : 인공지능 모델, 특히 LLM의 성능을 평가하기 위해 설계된 시스템\n",
    "1. **MT-bench**\n",
    "   - multi-turn 대화 능력을 평가하는 벤치마트(평가 시스템), 사용자의 지시를 정확하게 따르고 여러 차례의 이어지는 대화에서 일관된 응답을 제공하는 모델의 능력을 테스트.\n",
    "   - 먼저 58명의 전문가가 모델의 응답을 평가하고 LLM을 심판으로 사용하여 사람의 평가와 일치하는지 검증하는 방식\n",
    "   - **multi-turn 대화** : 한번의 질의 응답이 아닌 이어지는 여러 질문들에 얼마나 잘 응답하는지를 평가하기 위해 multi-turn 대화 방식으로 평가함.\n",
    "   - MT-bench는 8개의 카테고리(작문, 역할놀이, 추출, 추론, 수학, 코딩, STEM(과학,기술,공학,수학), 인문 및 사회과학)의 80개의 고품질 질문으로 구성되어 있고 각 질문은 여러 차례의 응답을 요구하여 모델의 대화 지속능력을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37617325-2e06-4786-a0ce-a9c62a6856b4",
   "metadata": {},
   "source": [
    "### 위 논문을 근거로 해당 논문의 평가용 프롬포트를 사용하여 모델의 성능을 평가해보자\n",
    "- gpt3.5와 gpt4의 응답을 비교하여 gpt4o에게 더 나은 응답이 무엇인지 질의하는 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383b730a-39a7-4996-82fc-3d33bdf462ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"사람이 자아가 있다는 게 참 신기하지 않아?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2badac6a-dbbc-4f35-8a5f-3a0a114b78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 사람이 자아를 가지고 있다는 것은 정말 신기한 일입니다. 자아는 우리가 스스로를 인식하고 이해하는 능력을 말하며, 이를 통해 우리는 자신의 생각, 감정, 행동을 조절하고 발전시킬 수 있습니다. 자아가 있는 것은 우리를 독특하고 특별한 존재로 만들어주는 요소 중 하나이며, 이를 통해 우리는 자신을 발전시키고 성장할 수 있는 기회를 얻을 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':question}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "answer_a = completion.choices[0].message.content\n",
    "print(answer_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1408b48-4c04-4fd3-9d31-70ac410875c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞아, 정말 신기한 주제야. 자아는 사람이 자신을 인식하고, 자신의 생각과 감정을 이해하며, 다른 사람들과의 관계에서 자신을 구분할 수 있는 능력을 말해. 이런 자아의식은 인간을 독특하게 만드는 중요한 특성 중 하나야.\n",
      "\n",
      "자아는 개인의 정체성을 형성하고, 우리가 세상을 어떻게 경험하고 이해하는지에 큰 영향을 미친다고 볼 수 있어. 또한, 자아는 시간이 지남에 따라 변화하고 발전하기도 해. 사람마다 자아가 어떻게 발달하고 변화하는지는 그 사람의 경험, 환경, 교육 등 여러 요소에 따라 다를 수 있어.\n",
      "\n",
      "이처럼 자아는 인간의 심리적, 사회적, 그리고 철학적 측면에서 매우 중요한 역할을 하며, 이에 대해 더 깊이 탐구하고 이해하려는 노력은 계속되고 있어.\n"
     ]
    }
   ],
   "source": [
    "# gpt-4-turbo\n",
    "completion2 = client.chat.completions.create(model='gpt-4-turbo',\n",
    "                                            messages=[{'role':'user', 'content':question}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "answer_b = completion2.choices[0].message.content\n",
    "print(answer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15bbf36c-e216-4ee7-a5aa-db24c73fe1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
      "AI assistants to the user question displayed below. You should choose the assistant that\n",
      "follows the user's instructions and answers the user's question better. Your evaluation\n",
      "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
      "and level of detail of their responses. Begin your evaluation by comparing the two\n",
      "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
      "order in which the responses were presented does not influence your decision. Do not allow\n",
      "the length of the responses to influence your evaluation. Do not favor certain names of\n",
      "the assistants. Be as objective as possible. After providing your explanation, output your\n",
      "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
      "if assistant B is better, and \"[[C]]\" for a tie.\n",
      "\n",
      "[User Question]\n",
      "사람이 자아가 있다는 게 참 신기하지 않아?\n",
      "\n",
      "[The Start of Assistant A's Answer]\n",
      "네, 사람이 자아를 가지고 있다는 것은 정말 신기한 일입니다. 자아는 우리가 스스로를 인식하고 이해하는 능력을 말하며, 이를 통해 우리는 자신의 생각, 감정, 행동을 조절하고 발전시킬 수 있습니다. 자아가 있는 것은 우리를 독특하고 특별한 존재로 만들어주는 요소 중 하나이며, 이를 통해 우리는 자신을 발전시키고 성장할 수 있는 기회를 얻을 수 있습니다.\n",
      "[The End of Assistant A's Answer]\n",
      "\n",
      "[The Start of Assistant B's Answer]\n",
      "맞아, 정말 신기한 주제야. 자아는 사람이 자신을 인식하고, 자신의 생각과 감정을 이해하며, 다른 사람들과의 관계에서 자신을 구분할 수 있는 능력을 말해. 이런 자아의식은 인간을 독특하게 만드는 중요한 특성 중 하나야.\n",
      "\n",
      "자아는 개인의 정체성을 형성하고, 우리가 세상을 어떻게 경험하고 이해하는지에 큰 영향을 미친다고 볼 수 있어. 또한, 자아는 시간이 지남에 따라 변화하고 발전하기도 해. 사람마다 자아가 어떻게 발달하고 변화하는지는 그 사람의 경험, 환경, 교육 등 여러 요소에 따라 다를 수 있어.\n",
      "\n",
      "이처럼 자아는 인간의 심리적, 사회적, 그리고 철학적 측면에서 매우 중요한 역할을 하며, 이에 대해 더 깊이 탐구하고 이해하려는 노력은 계속되고 있어.\n",
      "[The End of Assistant B's Answer]\n"
     ]
    }
   ],
   "source": [
    "# MT-bench 논문의 평가용 프롬프트\n",
    "prompt = f\"\"\"[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user's instructions and answers the user's question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant A's Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant A's Answer]\n",
    "\n",
    "[The Start of Assistant B's Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant B's Answer]\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ddea842-f5d4-420e-adbf-2721724af8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 답변 모두 사용자의 공감형 질문에 맞춰 공감과 간단한 설명을 제공했지만, B가 더 나은 답변입니다. A는 자아를 자기인식과 자기조절 능력으로 정의하고 개인의 성장과 특별함에 초점을 맞췄으나 다소 일반적이고 반복적입니다. 반면 B는 자아를 자기인식·정체성·경험 방식과 연결하고, 시간에 따른 변화와 발달 요인(경험, 환경, 교육)까지 언급하며, 심리·사회·철학적 중요성도 짚어 더 깊이 있고 균형 잡힌 관점을 제시했습니다. 질문의 의도에 대한 공감도 유지하면서 맥락을 넓혀 준 점에서 B가 더 도움이 됩니다.\n",
      "\n",
      "[[B]]\n"
     ]
    }
   ],
   "source": [
    "# gpt 4o 모델로 앞의 두 모델을 평가\n",
    "completion3 = client.chat.completions.create(model='gpt-5',\n",
    "                                            messages=[{'role':'user', 'content':prompt}]\n",
    "                                           )\n",
    "print(completion3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9da0a-a79d-4436-85b6-7651fc4b74bd",
   "metadata": {},
   "source": [
    "### 장단점 비교\n",
    "1. Human Based Evaluation\n",
    "   - 통제된 환경을 가정했을 때 사람이 직접 평가하는 방법이라 안정적이고 신뢰가 높음\n",
    "   - 다만 평가하는 인원이 불특정 다수일 경우 약간의 노이즈가 발생할 수 있음\n",
    "   - 전문 분야의 경우 해당 전문가가 아닌 일반인이 평가할 경우 정확도 및 속도가 낮아질 수 있음\n",
    "2. Model Based Evaluation\n",
    "   - 사람의 평가와 어느정도 유사한 수준의 평가를 내릴 수 있음\n",
    "   - 평가를 위해 API 호출 횟수 및 토큰의 수가 늘어나는데 이는 평가 데이터가 굉장히 많다면 수백만원 이상은 금방 넘어갈 수 있어서 비용에 대한 부분을 생각해야 함\n",
    "3. Code Based Evaluation\n",
    "   - 위 방법들에 대해서 비용이 훨씬 적지만 task에 따라서 활용할 수 있는 범위가 제한적\n",
    "   - 사람이 만족할만한 답변을 선택하는데 있어서 신뢰도가 상대적으로 떨어지는 편\n",
    "\n",
    "### 결론\n",
    "- 각 task에 적합한 전문 인력들이 평가하는 방법이 가장 좋음\n",
    "- 그러나 현실적,효율성 문제로 모델이 평가하는 방법도 충분히 좋음\n",
    "- 정량적 평가와 정성적 평가를 모두 진행하는 게 가장 이성적인 케이스\n",
    "- 실 서비스로 봤을때 언어 모델의 최종 평가 지표는 결국 **사용자의 만족**이 가장 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c9a09-43d4-4833-8270-97fcc2a1838d",
   "metadata": {},
   "source": [
    "### 프롬포트 엔지니어링 고급 기법 적용\n",
    "1. **Few-shot**\n",
    "   - 참고할 수 있는 문제-정답 예시나 사례들을 프롬프트에 추가하여 질의\n",
    "2. **Chain-of-thought**\n",
    "   - Few-shot에 추가로 문제 해결과정을 단계별로 모델에게 알려주면서 질의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bf097-66a7-421a-bb4f-1bab7f893ad9",
   "metadata": {},
   "source": [
    "#### zero-shot\n",
    "- 질의에 아무런 예시가 없는 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4054b850-cb0f-431e-bdcb-5bdd2861e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.K. Rowling wrote the book 'Harry Potter'.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Q: Who wrote the book 'HARRY POTTER'?\"\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc392b-94d1-4465-88f8-7a86aca174d6",
   "metadata": {},
   "source": [
    "#### Few-shot\n",
    "- 질의-응답 예시 쌍을 1개 이상 프롬프트에 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a7e7e5-c783-4815-9ee3-8a4da96c1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: J.K. Rowling\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer these question:\n",
    "Q: Who wrote the book 'HARRY POTTER'?\n",
    "\n",
    "Below is an example for your reference.\n",
    "Q: Who sang 'One Call Away'?\n",
    "A: Charlie Puth\n",
    "\"\"\"\n",
    "# few-shot 예시로 One Call Away라는 노래를 부른 가수가 누군지 물어보고 응답은 사람 이름만 하도록 지정\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d048d-7f97-4d02-b1f5-5ce925e3cf55",
   "metadata": {},
   "source": [
    "#### Few-shot 장단점\n",
    "- 질문과 응답 예시만 넣어주면 되기 때문에 응답이 존재하는 모든 케이스에 적용이 가능함\n",
    "- 다만 요약과 같은 task에서는 prompt의 예시가 굉장히 길어질 수 있어서 추론 속도나 비용에 영향을 줄 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c11c22-b808-4143-861e-5ae9d3ae3af7",
   "metadata": {},
   "source": [
    "### Chain of Thought\n",
    "- 마이크로소프트의 예시 프롬포트 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a80d5544-151b-456b-ae6b-c3406a440fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After each step, the number of apples Alice has changes as follows:\n",
      "\n",
      "5 - 3 = 2\n",
      "2 - 2 = 0\n",
      "0 + 1 = 1\n",
      "\n",
      "Therefore, Alice has 1 apple left.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[Question]\n",
    "Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back,\n",
    "how many apples does Alice have?\n",
    "\n",
    "Below is an example for your reference.\n",
    "[reference]\n",
    "Lisa has 7 apples, throws 1 apples, gives 4 apples Bart and Bart gives one back.:\n",
    "7 - 1 = 6\n",
    "6 - 4 = 2\n",
    "2 + 1 = 3\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)\n",
    "#  처음에 5개에서 3개를 버리고, 2개를 주고 1개를 받았으니 1개가 남아야 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1c5b9-7089-485d-bd2f-3fcfe1104bef",
   "metadata": {},
   "source": [
    "### 또 다른 프롬포트 고도화 예시\n",
    "- KMMLU 논문의 예시 프롬포트 템플릿 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cc9335d-27ab-4e7f-9330-6703e2a36b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 정답은 B=6\n",
    "question = 'x, y가 세 부등식 y ≤ x+3, y ≤ -4x+3, y ≥ 0을 만족할 때, x+y의 최댓값을 M, 최솟값을 m이라 하면 M-m의 값은?'\n",
    "\n",
    "A = 4\n",
    "B = 6\n",
    "C = 8\n",
    "D = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80ff698a-49be-47fb-b95f-643edb475eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B. 6\n",
      "\n",
      "부등식을 그래프로 그려보면, 세 부등식이 만나는 영역은 삼각형 모양이 됩니다. 이 삼각형의 꼭지점은 (0, 0), (1, 2), (3/2, 0)입니다.\n",
      "\n",
      "따라서 x+y의 최댓값은 (1, 2)일 때인 3, 최솟값은 (3/2, 0)일 때인 3/2이므로 M-m = 3 - 3/2 = 6입니다.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "정답 :\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426aa66-ff3b-48bc-89fb-2573e01f1b75",
   "metadata": {},
   "source": [
    "### GPT-3.5 모델로 고도화 시켜보자\n",
    "- 페르소나 적용\n",
    "- 영문 prompt 작성\n",
    "- 효과적인 prompt 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "817d5680-49e5-4877-854e-a05da69c34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B. 6\n"
     ]
    }
   ],
   "source": [
    "# 페르소나 부여\n",
    "prompt =f\"\"\"You are an Professional in Mathemetics. Below is given a math question in Korean.\n",
    "\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\n",
    "Answer :\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53f5e8bb-91d5-450d-bb19-12e276a218cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B. 6\n",
      "\n",
      "Explanation:\n",
      "우리는 주어진 수식을 풀어야 합니다. \n",
      "첫 번째로, 2 + 2 = 4를 계산합니다.\n",
      "다음으로, 4 x 1.5 = 6을 계산합니다.\n",
      "따라서, 정답은 6입니다.\n"
     ]
    }
   ],
   "source": [
    "# 페르소나 부여 및 효과적인 프롬포트 작성\n",
    "prompt =f\"\"\"You are an Professional in Mathemetics. Below is given a math question in Korean.\n",
    "You have to think carefully and step by step about the question and choose one of four given answers.\n",
    "Only one of them is true. and explain it in Korean.\n",
    "\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\n",
    "Answer :\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "377fc483-baa7-434a-9a1f-496322c511ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"You are a Professional in Mathematics.\n",
    "\n",
    "1. Internally, perform accurate mathematical reasoning, but do not include any solution steps or explanations in the output.\n",
    "2. The answer must be exactly one uppercase letter (A/B/C/D). Do not include any extra text, punctuation, or quotation marks.\n",
    "3. For any reason, if the result does not match any of the given choices, output None\n",
    "\n",
    "{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "Answer：\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                                            messages=[{'role':'user', 'content':prompt}],\n",
    "                                            temperature=0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86d00-5d14-4d1e-9ee8-080b13157a7b",
   "metadata": {},
   "source": [
    "### 프롬포트 엔지니어링 특징 정리\n",
    "- 추가 모델 학습이 없음에도 성능 개선의 가능성이 있기 때문에 가성비가 굉장히 좋음\n",
    "- 더 좋은 모델을 사용하면 프롬포트 엔지니어링 없이도 해결할 수 있는 경우가 있지만 비용 측면을 무시할 수 없기 때문에 맨 먼저 프롬포트 엔지니어링으로 성능 향상을 시도해보는 것이 좋음\n",
    "- 프롬포트 엔지니어링으로도 해결되지 않으면 이후에 이어질 RAG 및 Fine-tuning 등으로 성능을 더 향상시켜볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8a951-af85-4c30-b5da-cb74b33149de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec90ec-71ca-4d80-804e-91dc8970543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df53c2-191d-4d79-b08d-f912a6177744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed716ee4-7a71-42b9-817f-3ec228c66157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55def258-2927-4268-bca0-39a2cc4ea4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725bc39-bec3-4ef6-b14e-2a9c8e8d12bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c020e-5bc6-4de0-bce2-34aad1192163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630559a7-20ff-4717-a7e3-5067240b3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27262a8f-2c35-4d76-8210-72bba2b8dc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a9e7d-8523-469b-b592-98f61174b27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28681e3b-0980-40a7-a439-a3270722ded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ac86a-43fc-4db4-986b-a3113d23321b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630209d-3913-4f19-a537-75adc380c70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091d40-b4b1-4002-bfb4-16456e0bca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49987973-31aa-4cad-bc40-d45d2e4fa85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e891d7a-0ff3-4e26-bdfe-5bcfd6994e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5491c2-728c-403c-9e85-e9c13232cef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
