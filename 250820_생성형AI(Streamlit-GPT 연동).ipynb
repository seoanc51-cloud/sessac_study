{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01723a21-3a32-461f-b1ba-85159e8103af",
   "metadata": {},
   "source": [
    "## GPT와 streamlit을 연동해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42558830-06ed-4793-81cc-40a81cb063bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-decouple\n",
      "  Downloading python_decouple-3.8-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\n",
      "Installing collected packages: python-decouple\n",
      "Successfully installed python-decouple-3.8\n"
     ]
    }
   ],
   "source": [
    "# API Key, DB 자격 증명 및 환경 변수와 같은 구성 설정을 안전하게 관리하는 python 패키지 설치\n",
    "!pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68dd7c8-e811-4f08-bb33-61d569685ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\82108\\anaconda3\\lib\\site-packages (0.3.28)\n",
      "Requirement already satisfied: pydantic in c:\\users\\82108\\anaconda3\\lib\\site-packages (2.10.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-openai) (0.3.72)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-openai) (1.99.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from pydantic) (2.27.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\82108\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\82108\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82108\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\82108\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.86.0->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48692cab-bce2-4122-8fdc-7457b8b509d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting module/myChatApp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/myChatApp.py\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_model = ChatOpenAI(model_name='gpt-3.5-turbo',                # fine-tuning된 모델명 넣어도 됨\n",
    "                        api_key = st.secrets[\"OPENAI_API_KEY\"],    # secrets.toml파일에 저장되어 있는 key값으로 value에 접근\n",
    "                        temperature=0.5\n",
    "                       )\n",
    "\n",
    "my_prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"],\n",
    "                        template=\"\"\"You are a AI assistant.\n",
    "                        You are currently having a conversation with a human.\n",
    "                        Answer the questions.\n",
    "                        chat_history: {chat_history},\n",
    "                        Human: {question}\n",
    "                        AI assistant:\n",
    "                        \"\"\"\n",
    "                       )\n",
    "\n",
    "# 일반적인 코드에서는 memory 객체를 생성하면 대화 내용들을 기억하지만 streamlit에서는 웹 서버에서 요청, 응답을 수행하기 때문에\n",
    "# 세션에 저장하지 않으면 다 초기화가 됨(따라서 memory 객체를 세션에서 관리해주는 코드를 작성해야 함)\n",
    "if \"pre_memory\" not in st.session_state :\n",
    "    # 세션에 pre_memory라는 key의 value값으로 ConversationBufferMemory객체를 저장해서 활용\n",
    "    st.session_state['pre_memory'] = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                              return_messages=True\n",
    "                                                             )\n",
    "\n",
    "llm_chain = LLMChain(llm=chat_model,\n",
    "                     prompt= my_prompt,\n",
    "                     memory=st.session_state['pre_memory']   # 세션으로 관리하고 있는 메모리 클래스 입력\n",
    "                    )\n",
    "\n",
    "# ChatGPT 서비스와 유사하게 웹 상에서 우리의 질의 응답 내역이 계속 보여져야 하기 때문에 세션관리가 필요\n",
    "if \"messages\" not in st.session_state :\n",
    "    st.session_state[\"messages\"] = [\n",
    "        # 맨 처음 웹 화면을 켰을때 출력될 수 있는 기본 문구를 설정\n",
    "        {\"role\":\"assistant\", \"content\":\"안녕하세요, 저는 AI Assistant 입니다 :\"}\n",
    "    ]\n",
    "\n",
    "# ============================================웹 화면 표시부분===============================================\n",
    "\n",
    "st.title(\"나의 작고 소중한 GPT 챗봇ㅋ\")\n",
    "\n",
    "# 반복문으로 messages에 있는 모든 대화 기록에 접근\n",
    "for message in st.session_state[\"messages\"] :\n",
    "    # chat_message : 메시지의 발신자 role(assistant인지 user인지)에 따라 UI를 구분하여 메시지 창을 표시해주는 함수\n",
    "    with st.chat_message(message[\"role\"]) :   # 세션에 저장된 각 message의 role에 접근(맨 처음에는 assistant만 있는 상태)\n",
    "        st.write(message[\"content\"])          # 해당 role에 맞는 content값 출력\n",
    "\n",
    "# chat_input : 채팅 메시지를 입력하는 UI를 생성하고 사용자가 입력한 텍스트를 변수에 저장\n",
    "user_prompt = st.chat_input(\"사용자 입력 ><\")\n",
    "\n",
    "# 사용자가 텍스트를 입력했을 때\n",
    "if user_prompt is not None :\n",
    "    st.session_state['messages'].append({\"role\":\"user\", \"content\":user_prompt})\n",
    "    with st.chat_message(\"user\") :      # user로 메시지 창 UI 출력\n",
    "        st.write(user_prompt)           # 사용자가 입력한 택스트 출력\n",
    "        \n",
    "# assistant가 응답하는 코드\n",
    "# 마지막 메시지의 role이 assistant가 아니라면 새로운 답변을 생성하고 세션에 추가\n",
    "# (마지막 메시지를 보는 것은 assistant가 본인의 응답에도 계속 대단하는 자문자답을 방지하기 위한 용도)\n",
    "if st.session_state[\"messages\"][-1][\"role\"] != \"assistant\" :\n",
    "    with st.chat_message(\"assistant\") :\n",
    "        # llM이 응답을 생성하는 동안 로딩 애니메이션 실행\n",
    "        with st.spinner(\"응답 생성중...\") :\n",
    "            try :\n",
    "                ai_response = llm_chain.predict(question=user_prompt)  # llm 응답 생성  \n",
    "                st.write(ai_response)                                  # 응답 출력\n",
    "                st.session_state[\"messages\"].append({\"role\":\"assistant\", \"content\":ai_response})  # 세션에 응답 내용 저장\n",
    "\n",
    "            # 발생한 에러 출력\n",
    "            except Exception as e :\n",
    "                st.error(f\"LLM 에러 발생 : {e}\")\n",
    "\n",
    "\n",
    "#==========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83bc04-136e-49eb-869d-9bc4ecec396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964720d-b6c3-4279-b160-56949f935ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f567552-1192-413d-8b82-0949c378bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6e92f-b3f8-4064-9508-753954d70393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddda51-043a-44fe-955d-f350b8485517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cd5ec-cbc5-4b16-a276-e259ad6fcb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ce01c-845b-47ba-becd-179d70c18921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc5ecb-84c5-42d9-9296-0e998a811a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7062a6f-71df-47ca-841f-349025e69ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a0185-eb6d-4586-ace4-e241a5a47928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
